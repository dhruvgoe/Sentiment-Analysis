{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruvgoe/Sentiment-Analysis/blob/main/Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plan of Action\n",
        "\n",
        "\n",
        "1.   Load **IMDb Movie Reviews dataset (50,000 reviews)**\n",
        "2.   **Pre-process dataset** by removing special characters, numbers, etc. from user reviews + convert **sentiment labels** positive & negative to numbers 1 & 0, respectively\n",
        "3.   **Import GloVe Word Embedding** to build Embedding Dictionary + Use this to build Embedding Matrix for our Corpus\n",
        "4. Model Training using **Deep Learning in Keras** for **LSTM Model** and analyse model performance and results\n",
        "4. Last, perform **predictions on real IMDb movie reviews**"
      ],
      "metadata": {
        "id": "5BaF_P5sfKla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3xHoFumSlRt",
        "outputId": "e1a318fa-f17f-4d12-bf3d-f6863ad35c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/IMDB\n",
        "! ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_kEVTjopUnh",
        "outputId": "471af818-9602-432b-eabb-6605c25df7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/IMDB\n",
            "a1_IMDB_Dataset.csv   a3_IMDb_Unseen_Reviews.csv  c2_IMDb_Unseen_Predictions.csv\n",
            "a2_glove.6B.100d.txt  c1_lstm_model_acc_0.862.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56_jc5wRTO9U",
        "outputId": "6eee35f2-a510-4fad-9ad5-27f8a34b54bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from numpy import array\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Dense\n",
        "from tensorflow.keras.layers import Flatten, GlobalMaxPooling1D, Embedding, Conv1D, LSTM\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "AIZ1Np_STAAe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset"
      ],
      "metadata": {
        "id": "ICaW9_gNS1BW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrQO95_ixPf0"
      },
      "outputs": [],
      "source": [
        "movie_reviews = pd.read_csv(\"/content/drive/MyDrive/IMDB/a1_IMDB_Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews.shape"
      ],
      "metadata": {
        "id": "y_uIRSIkT851"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews.head(5)"
      ],
      "metadata": {
        "id": "e_2INZ1rUAYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for missing values\n",
        "\n",
        "movie_reviews.isnull().values.any()"
      ],
      "metadata": {
        "id": "Q1KlIK70UDYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's observe distribution of positive / negative sentiments in dataset\n",
        "\n",
        "import seaborn as sns\n",
        "sns.countplot(x='sentiment', data=movie_reviews)"
      ],
      "metadata": {
        "id": "kuDReDncUFyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "LEhPPxwYUL0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_reviews[\"review\"][2]\n",
        "\n",
        "# We can see that our text contains punctuations, brackets, HTML tags and numbers\n",
        "# We will preprocess this text in the next section"
      ],
      "metadata": {
        "id": "NrLe4HfSUJCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    '''Removes HTML tags: replaces anything between opening and closing <> with empty space'''\n",
        "\n",
        "    return TAG_RE.sub('', text)"
      ],
      "metadata": {
        "id": "fl_47ozDUR4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "Mf3mXpdJUcqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sen):\n",
        "    '''Cleans text data up, leaving only 2 or more char long non-stepwords composed of A-Z & a-z only\n",
        "    in lowercase'''\n",
        "\n",
        "    sentence = sen.lower()\n",
        "\n",
        "    # Remove html tags\n",
        "    sentence = remove_tags(sentence)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removalvocab_length\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)  # When we remove apostrophe from the word \"Mark's\", the apostrophe is replaced by an empty space. Hence, we are left with single character \"s\" that we are removing here.\n",
        "\n",
        "    # Remove multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)  # Next, we remove all the single characters and replace it by a space which creates multiple spaces in our text. Finally, we remove the multiple spaces from our text as well.\n",
        "\n",
        "    # Remove Stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "    sentence = pattern.sub('', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "6gyD-L_2Uek8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling preprocessing_text function on movie_reviews\n",
        "\n",
        "X = []\n",
        "sentences = list(movie_reviews['review'])\n",
        "for sen in sentences:\n",
        "    X.append(preprocess_text(sen))"
      ],
      "metadata": {
        "id": "nw8uKlWuUn0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample cleaned up movie review\n",
        "\n",
        "X[2]"
      ],
      "metadata": {
        "id": "lsynQYXxUrbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting sentiment labels to 0 & 1\n",
        "\n",
        "y = movie_reviews['sentiment']\n",
        "\n",
        "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))"
      ],
      "metadata": {
        "id": "F4cOtsxXU2sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# The train set will be used to train our LSTM model\n",
        "# while test set will be used to evaluate how well our model performs"
      ],
      "metadata": {
        "id": "iVKKVenBU5yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing embedding layer\n",
        "\n",
        "Let's now write the script for our embedding layer. Embedding layer converts our textual data into numeric form. It is then **used as the first layer for the deep learning models in Keras**.\n"
      ],
      "metadata": {
        "id": "WeX3CqEdVLjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer expects the words to be in numeric form\n",
        "# Using Tokenizer function from keras.preprocessing.text library\n",
        "# Method fit_on_text trains the tokenizer\n",
        "# Method texts_to_sequences converts sentences to their numeric form\n",
        "\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train = word_tokenizer.texts_to_sequences(X_train)\n",
        "X_test = word_tokenizer.texts_to_sequences(X_test)"
      ],
      "metadata": {
        "id": "Ko4r-LcOVJNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding 1 to store dimensions for words for which no pretrained word embeddings exist\n",
        "\n",
        "vocab_length = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "vocab_length"
      ],
      "metadata": {
        "id": "Lq7zi3VhYmkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding all reviews to fixed length 100\n",
        "\n",
        "maxlen = 100\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "u0vuVS0chtiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe word embeddings and create an Embeddings Dictionary\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "\n",
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/MyDrive/IMDB/a2_glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "glove_file.close()"
      ],
      "metadata": {
        "id": "3BrloUidhvq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Embedding Matrix having 100 columns\n",
        "# Containing 100-dimensional GloVe word embeddings for all words in our corpus.\n",
        "\n",
        "embedding_matrix = zeros((vocab_length, 100))\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    print(embedding_vector)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector"
      ],
      "metadata": {
        "id": "ychwsPjSiP-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM"
      ],
      "metadata": {
        "id": "GXnQTSdhoeqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM"
      ],
      "metadata": {
        "id": "hGloP8KAk0YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network architecture\n",
        "\n",
        "lstm_model = Sequential()\n",
        "embedding_layer = Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)\n",
        "\n",
        "lstm_model.add(embedding_layer)\n",
        "lstm_model.add(LSTM(100))\n",
        "\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "uvHFoBRuokBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compiling\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "print(lstm_model.summary())\n"
      ],
      "metadata": {
        "id": "MrhJDVLvolTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "\n",
        "lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "RoECONuGoows"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions on the Test Set\n",
        "\n",
        "score = lstm_model.evaluate(X_test, y_test, verbose=1)"
      ],
      "metadata": {
        "id": "ey31oaYposOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Performance\n",
        "\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "bBMTybdPowyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Performance Charts\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(lstm_model_history.history['acc'])\n",
        "plt.plot(lstm_model_history.history['val_acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm_model_history.history['loss'])\n",
        "plt.plot(lstm_model_history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9Se9CKJNoz-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model as a h5 file for possible use later\n",
        "\n",
        "lstm_model.save(f\"./c1_lstm_model_acc_{round(score[1], 3)}.h5\", save_format='h5')"
      ],
      "metadata": {
        "id": "0O2c1tAKpoK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Predictions on Live IMDb data"
      ],
      "metadata": {
        "id": "09tUO_pVpIkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls # lists files in working directory"
      ],
      "metadata": {
        "id": "QyTfkai5o2ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sample IMDb reviews csv, having ~6 movie reviews, along with their IMDb rating\n",
        "\n",
        "sample_reviews = pd.read_csv(\"a3_IMDb_Unseen_Reviews.csv\")\n",
        "\n",
        "sample_reviews.head(6)\n"
      ],
      "metadata": {
        "id": "5c_elQHnpvAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess review text with earlier defined preprocess_text function\n",
        "\n",
        "unseen_reviews = sample_reviews['Review Text']\n",
        "\n",
        "unseen_processed = []\n",
        "for review in unseen_reviews:\n",
        "  review = preprocess_text(review)\n",
        "  unseen_processed.append(review)"
      ],
      "metadata": {
        "id": "TzQkj4k5p74b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenising instance with earlier trained tokeniser\n",
        "unseen_tokenized = word_tokenizer.texts_to_sequences(unseen_processed)\n",
        "\n",
        "# Pooling instance to have maxlength of 100 tokens\n",
        "unseen_padded = pad_sequences(unseen_tokenized, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "id": "2Of9_rNBp_iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing tokenised instance to the LSTM model for predictions\n",
        "unseen_sentiments = lstm_model.predict(unseen_padded)\n",
        "\n",
        "unseen_sentiments"
      ],
      "metadata": {
        "id": "Z3MA4-EhqA0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing model output file back to Google Drive\n",
        "\n",
        "sample_reviews['Predicted Sentiments'] = np.round(unseen_sentiments*10,1)\n",
        "\n",
        "df_prediction_sentiments = pd.DataFrame(sample_reviews['Predicted Sentiments'], columns = ['Predicted Sentiments'])\n",
        "df_movie                 = pd.DataFrame(sample_reviews['Movie'], columns = ['Movie'])\n",
        "df_review_text           = pd.DataFrame(sample_reviews['Review Text'], columns = ['Review Text'])\n",
        "df_imdb_rating           = pd.DataFrame(sample_reviews['IMDb Rating'], columns = ['IMDb Rating'])\n",
        "\n",
        "\n",
        "dfx=pd.concat([df_movie, df_review_text, df_imdb_rating, df_prediction_sentiments], axis=1)\n",
        "\n",
        "dfx.to_csv(\"./c2_IMDb_Unseen_Predictions.csv\", sep=',', encoding='UTF-8')\n",
        "\n",
        "dfx.head(6)"
      ],
      "metadata": {
        "id": "eFwM4N5CqB8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yNnBpWMVqGxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}